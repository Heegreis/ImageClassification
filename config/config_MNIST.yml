# parameters configuration
mode: train
root: data/
predefined_dataset: MNIST
classes:
  [
    "0 - zero",
    "1 - one",
    "2 - two",
    "3 - three",
    "4 - four",
    "5 - five",
    "6 - six",
    "7 - seven",
    "8 - eight",
    "9 - nine",
  ]
max_samples: null
batch_size: 32
num_workers: 0
device: cuda
lr: 1e-3
model_name: tf_mobilenetv3_small_minimal_100
in_chans: 1
loss_function_name: BCEWithLogitsLoss
data_balance: False
checkpoint_path: null
seed: 0
early_stopping: True
patience: 3
default_root_dir: save/
gpus: 1
precision: 32
max_epochs: 100

# transforms configuration
transforms_config:
  train:
    Resize:
      - 224
      - 224
    ColorJitter:
    RandomRotation: 90
    ToTensor:
    RandomErasing:

  val:
    Resize:
      - 224
      - 224
    ToTensor:

  test:
    Resize:
      - 224
      - 224
    ToTensor:

  predict:
    Resize:
      - 224
      - 224
    ToTensor:

# target transforms configuration
target_transforms_config:
  train:
    LabelSmoothing:
      alpha: 0.2
      num_classes: null

  val:
    OneHotEncoder:
      num_classes: null

  test:
    OneHotEncoder:
      num_classes: null

  predict:
    OneHotEncoder:
      num_classes: null

# optimizers configuration
optimizers_config:
  Adam:
    betas:
      - 0.9
      - 0.999
    eps: 1e-08
    weight_decay: 0
    amsgrad: False

# learning rate schedulers configuration
lr_schedulers_config:
  CosineAnnealingLR:
    T_max: 10
